from langchain.callbacks.base import BaseCallbackHandler

class StreamingHandler(BaseCallbackHandler):
    """
    A callback handler for streaming tokens from a language model to a queue.
    
    Attributes:
        queue (Queue): The queue to put tokens into.
        streaming_run_ids (set): A set of run IDs that are currently streaming.
    """
    def __init__(self, queue):
        self.queue = queue
        self.streaming_run_ids = set()

    def on_chat_model_start(self, serialized, messages, run_id, **kwargs):
        """
        Called when a chat model starts processing.
        
        Args:
            serialized (dict): Serialized chat model data.
            messages (list): List of messages.
            run_id (str): The run ID.
        """
        if serialized["kwargs"]["streaming"]:
            self.streaming_run_ids.add(run_id)

    def on_llm_new_token(self, token, **kwargs):
        """
        Called when a new token is generated by the language model.
        
        Args:
            token (str): The generated token.
        """
        self.queue.put(token)

    def on_llm_end(self, response, run_id, **kwargs):
        """
        Called when the language model finishes processing.
        
        Args:
            response (dict): The response from the language model.
            run_id (str): The run ID.
        """
        if run_id in self.streaming_run_ids:
            self.queue.put(None)
            self.streaming_run_ids.remove(run_id)

    def on_llm_error(self, error, **kwargs):
        """
        Called when an error occurs during language model processing.
        
        Args:
            error (Exception): The error that occurred.
        """
        self.queue.put(None)